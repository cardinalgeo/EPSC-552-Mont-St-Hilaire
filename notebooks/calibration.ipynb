{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import json\n",
    "from scipy.stats import linregress\n",
    "from statsmodels.stats.weightstats import ztest as ztest\n",
    "\n",
    "# local code\n",
    "from source.interactive_plots import interactive_linear_regression_calibration_plot\n",
    "from source.get_elements      import get_elements\n",
    "from source.outliers          import dixon_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from XRF analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrf_data = pd.read_csv(\"../data/interim/xrf_data_clean.csv\") # load xrf data\n",
    "# drop uncertainty columns \n",
    "xrf_data.drop([column for column in xrf_data.columns if column.endswith(\"+/-\")], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data for standard reference materials and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "srm_data = pd.read_csv(\"../data/interim/standard_reference_material_certified_values.csv\") # load SRM data\n",
    "srm_data.drop([\"1SD\", \"95% Confidence Low\", \"95% Confidence High\"], axis=1, inplace=True) # drop unnecessary columns\n",
    "\n",
    "# clean SRM data\n",
    "for row in srm_data.iterrows(): \n",
    "    row[1][\"Sample ID\"] = row[1][\"Sample ID\"].lower()\n",
    "    row[1][\"Analyte\"] = row[1][\"Analyte\"].split(\",\",1)[0]\n",
    "\n",
    "    # clean certified value\n",
    "    cert_val = row[1][\"Certified Value\"]\n",
    "    if cert_val.startswith(\"<\"): \n",
    "        row[1][\"Certified Value\"] = float(cert_val.lstrip(\"< \")) / 2 # replace BDL with half value\n",
    "    else:\n",
    "        row[1][\"Certified Value\"] = float(row[1][\"Certified Value\"])\n",
    "\n",
    "    # clean units\n",
    "    row[1][\"Units\"] = row[1][\"Units\"].lstrip(\"(\").rstrip(\")\")\n",
    "    if row[1][\"Units\"] == \"wt.%\":  # convert units\n",
    "       row[1][\"Certified Value\"] = row[1][\"Certified Value\"] * 1e4\n",
    "       row[1][\"Units\"] = \"ppm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove standard data for a given element that is unsuitable for calibration\n",
    "\n",
    "Compare the measured concentration of each element for a given standard to the distribution of measured concentrations for that element across all non-standards. If the measured concentration \n",
    "\n",
    "    1.) falls outside the range of the distribution \n",
    "    2.) is more than three standard deviations from the mean \n",
    "\n",
    "that value is set to `NaN` (all values set to `NaN` are filtered out during calibration). \n",
    "\n",
    "Note: I should consider conducting a test to determine if each distribution is normally or log-normally distributed; if the latter, I should log-transform the data and then calculate the z-score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score(data, value): \n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "\n",
    "    return (value - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oreas24b, 2021-10-06, K, 31090.0, 6.471422612919988\n",
      "oreas24b, 2021-10-14, K, 31369.0, 6.563395682670596\n",
      "oreas24b, 2021-10-21, K, 31367.0, 6.562736377511094\n",
      "oreas24b, 2021-10-29, K, 31590.0, 6.6362489027956295\n",
      "oreas24b, 2021-10-31, K, 31006.0, 6.44373179622088\n",
      "oreas24c, 2021-10-21, Cr, 248.0, 7.647141304308473\n",
      "oreas901, 2021-10-06, As, 70.0, 5.172844445093555\n",
      "oreas901, 2021-10-06, Cu, 1275.0, 36.20572840108871\n",
      "oreas901, 2021-10-06, K, 41054.0, 9.756080917561775\n",
      "oreas901, 2021-10-22, Cu, 1270.0, 36.06111662068375\n",
      "oreas901, 2021-10-22, K, 40904.0, 9.706633030599082\n",
      "oreas501b, 2021-10-06, Cr, 275.0, 8.764381497151398\n",
      "oreas501b, 2021-10-06, Cu, 2267.0, 64.89670563343348\n",
      "oreas501b, 2021-10-06, K, 36229.0, 8.165507220261834\n",
      "oreas501b, 2021-10-06, Mo, 104.5, 12.065017729332341\n",
      "oreas501b, 2021-10-06, Rb, 183.4, 5.275784103193051\n",
      "oreas501b, 2021-10-22, Cr, 280.0, 8.971277829159346\n",
      "oreas501b, 2021-10-22, Cu, 2301.0, 65.88006574018723\n",
      "oreas501b, 2021-10-22, K, 37039.0, 8.432525809860373\n",
      "oreas501b, 2021-10-22, Mo, 101.9, 11.75002529966039\n",
      "oreas501b, 2021-10-22, Rb, 185.5, 5.361027615228126\n",
      "oreas45e, 2021-10-06, Cl, 8891.0, 9.864204654868448\n",
      "oreas45e, 2021-10-06, Co, 158.0, 17.02261459710877\n",
      "oreas45e, 2021-10-06, Cr, 1497.0, 59.32984503989414\n",
      "oreas45e, 2021-10-06, Cu, 697.0, 19.488606586274923\n",
      "oreas45e, 2021-10-06, Fe, 512099.0, 17.947559114596007\n",
      "oreas45e, 2021-10-14, Cl, 8743.0, 9.674013465185132\n",
      "oreas45e, 2021-10-14, Co, 153.0, 16.42183292690372\n",
      "oreas45e, 2021-10-14, Cr, 1529.0, 60.65398156474502\n",
      "oreas45e, 2021-10-14, Cu, 689.0, 19.25722773762698\n",
      "oreas45e, 2021-10-14, Fe, 512835.0, 17.976245114862778\n",
      "oreas45e, 2021-10-21, Cl, 8489.0, 9.34760426126917\n",
      "oreas45e, 2021-10-21, Co, 154.0, 16.54198926094473\n",
      "oreas45e, 2021-10-21, Cr, 1500.0, 59.45398283909891\n",
      "oreas45e, 2021-10-21, Cu, 701.0, 19.60429601059889\n",
      "oreas45e, 2021-10-21, Fe, 510931.0, 17.902035679390046\n",
      "oreas45e, 2021-10-22, Cl, 9115.0, 10.152061590605358\n",
      "oreas45e, 2021-10-22, Co, 152.0, 16.30167659286271\n",
      "oreas45e, 2021-10-22, Cr, 1513.0, 59.991913302319574\n",
      "oreas45e, 2021-10-22, Cu, 690.0, 19.28615009370797\n",
      "oreas45e, 2021-10-22, Fe, 513678.0, 18.009101498320504\n",
      "oreas45e, 2021-10-31, Cl, 8894.0, 9.868059881686353\n",
      "oreas45e, 2021-10-31, Co, 143.0, 15.220269586493622\n",
      "oreas45e, 2021-10-31, Cr, 1477.0, 58.502259711862344\n",
      "oreas45e, 2021-10-31, Cu, 682.0, 19.05477124506003\n",
      "oreas45e, 2021-10-31, Fe, 517798.0, 18.169680738944265\n",
      "oreas902, 2021-10-06, As, 499.0, 47.048871623577675\n",
      "oreas902, 2021-10-06, Au, 20.8, 7.7050765543208\n",
      "oreas902, 2021-10-06, Cu, 2403.0, 68.8301460604485\n",
      "oreas902, 2021-10-06, K, 39307.0, 9.180177860736283\n",
      "oreas902, 2021-10-06, S, 15864.0, 13.08656918658384\n",
      "oreas902, 2021-10-13, As, 505.0, 47.634550325374654\n",
      "oreas902, 2021-10-13, Au, 15.6, 5.5032900622297385\n",
      "oreas902, 2021-10-13, Cu, 2399.0, 68.71445663612452\n",
      "oreas902, 2021-10-13, K, 39299.0, 9.177540640098274\n",
      "oreas902, 2021-10-13, S, 15998.0, 13.217445350713227\n",
      "oreas503b, 2021-10-06, Cr, 245.0, 7.523003505103703\n",
      "oreas503b, 2021-10-06, Cu, 4687.0, 134.88880734943587\n",
      "oreas503b, 2021-10-06, K, 37869.0, 8.706137451053937\n",
      "oreas503b, 2021-10-06, Mo, 335.0, 39.990308129095745\n",
      "oreas503b, 2021-10-06, S, 9715.0, 7.080915953512236\n",
      "oreas503b, 2021-11-04, Cu, 4656.0, 133.9922143109251\n",
      "oreas503b, 2021-11-04, K, 37316.0, 8.523839574451477\n",
      "oreas503b, 2021-11-04, Mo, 337.0, 40.23260999807417\n",
      "oreas503b, 2021-11-04, S, 9441.0, 6.813303498501402\n",
      "oreas904, 2021-10-06, As, 93.0, 7.417946135315315\n",
      "oreas904, 2021-10-06, Cu, 5643.0, 162.5385797628649\n",
      "oreas904, 2021-10-06, K, 42488.0, 10.228802716925115\n",
      "oreas904, 2021-10-13, As, 91.0, 7.222719901382988\n",
      "oreas904, 2021-10-13, Cu, 5643.0, 162.5385797628649\n",
      "oreas904, 2021-10-13, K, 42782.0, 10.325720575371992\n",
      "oreas502b, 2021-10-06, Cr, 275.0, 8.764381497151398\n",
      "oreas502b, 2021-10-06, Cu, 6871.0, 198.05523303032396\n",
      "oreas502b, 2021-10-06, K, 38138.0, 8.794813995007033\n",
      "oreas502b, 2021-10-06, Mo, 247.0, 29.329025894045078\n",
      "oreas502b, 2021-10-06, S, 14300.0, 11.559029479879664\n",
      "oreas502b, 2021-10-29, Cr, 271.0, 8.598864431545039\n",
      "oreas502b, 2021-10-29, Cu, 6850.0, 197.44786355262312\n",
      "oreas502b, 2021-10-29, K, 37635.0, 8.628998747392137\n",
      "oreas502b, 2021-10-29, Mo, 247.0, 29.329025894045078\n",
      "oreas502b, 2021-10-29, S, 13343.0, 10.624339262925764\n",
      "oreas504b, 2021-10-06, Cu, 9909.0, 285.92135080437987\n",
      "oreas504b, 2021-10-06, K, 39535.0, 9.255338648919576\n",
      "oreas504b, 2021-10-06, Mo, 508.0, 60.94941979572945\n",
      "oreas504b, 2021-10-06, S, 19250.0, 16.393634050330864\n",
      "oreas504b, 2021-10-06, Se, 15.6, 7.532624901122155\n",
      "oreas504b, 2021-10-07, Cu, 9994.0, 288.37975107126425\n",
      "oreas504b, 2021-10-07, K, 40465.0, 9.561915548088269\n",
      "oreas504b, 2021-10-07, Mo, 515.0, 61.79747633715393\n",
      "oreas504b, 2021-10-07, S, 18302.0, 15.46773402350506\n",
      "oreas504b, 2021-10-07, Se, 14.6, 7.186739063825729\n",
      "oreas504b, 2021-11-04, Au, 18.0, 6.519499212425613\n",
      "oreas504b, 2021-11-04, Cr, -1238.0, -53.84244856845399\n",
      "oreas504b, 2021-11-04, Cu, 9720.0, 280.45502550507223\n",
      "oreas504b, 2021-11-04, Mo, 521.0, 62.524381944089214\n",
      "oreas504b, 2021-11-04, Sn, 30.0, 7.215858304012305\n",
      "oreas504b, 2021-11-04, V, -2360.0, -51.07117640573399\n",
      "oreas45d, 2021-10-06, Cl, 5608.0, 5.645301440474354\n",
      "oreas45d, 2021-10-06, Co, 79.4, 7.5783267414854\n",
      "oreas45d, 2021-10-06, Cr, 791.0, 30.11608296037174\n",
      "oreas45d, 2021-10-06, Fe, 231342.0, 7.004902352507182\n",
      "oreas45d, 2021-10-07, Cl, 5460.0, 5.455110250791037\n",
      "oreas45d, 2021-10-07, Co, 79.5, 7.5903423748895005\n",
      "oreas45d, 2021-10-07, Cr, 776.0, 29.495393964347894\n",
      "oreas45d, 2021-10-07, Fe, 231003.0, 6.991689643145178\n",
      "oreas45d, 2021-11-04, Cl, 5508.0, 5.516793879877518\n",
      "oreas45d, 2021-11-04, Co, 77.7, 7.374060973615683\n",
      "oreas45d, 2021-11-04, Cr, 764.0, 28.998842767528814\n",
      "oreas45d, 2021-11-04, Fe, 231185.0, 6.998783192124189\n",
      "oreas903, 2021-10-06, Cu, 5719.0, 164.73667882502036\n",
      "oreas903, 2021-10-06, K, 40038.0, 9.421153896534472\n"
     ]
    }
   ],
   "source": [
    "non_standards_data = xrf_data.loc[xrf_data[\"qaqc_type\"]!=\"standard\"]\n",
    "\n",
    "outlier_stddev_cutoff = 5\n",
    "\n",
    "for standard in xrf_data.loc[xrf_data[\"qaqc_type\"]==\"standard\", \"sample_id\"].unique(): \n",
    "    for date in xrf_data.loc[xrf_data[\"sample_id\"]==standard, \"date\"].unique():\n",
    "        for element in get_elements(xrf_data.columns.to_list()): \n",
    "            standard_date_element = xrf_data.loc[(xrf_data[\"sample_id\"]==standard) & (xrf_data[\"date\"]==date)][element].values\n",
    "\n",
    "            if (non_standards_data[element] > standard_date_element[0]).all() | \\\n",
    "               (non_standards_data[element] < standard_date_element[0]).all(): \n",
    "                \n",
    "                # print(non_standards_data[element].to_numpy())\n",
    "                z = z_score(non_standards_data[element], standard_date_element[0])\n",
    "                if abs(z) > outlier_stddev_cutoff: \n",
    "                    print(standard + \", \" + date + \", \" + element + \", \" + str(standard_date_element[0]) + \", \" + str(z))\n",
    "\n",
    "                    xrf_data.loc[(xrf_data[\"sample_id\"]==standard) & (xrf_data[\"date\"]==date), element] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the elements analyzed by the XRF and for which concentrations are reported for one or more standard reference materials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = get_elements(\n",
    "                        list(\n",
    "                            set(srm_data[\"Analyte\"].unique()) & \\\n",
    "                            set(xrf_data.columns.to_list())\n",
    "                            )\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a linear regression model for each element in order to predict the true concentration from the measured concentration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertcollar/miniconda3/envs/EPSC-552-Mont-St-Hilaire/lib/python3.9/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/Users/robertcollar/miniconda3/envs/EPSC-552-Mont-St-Hilaire/lib/python3.9/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n"
     ]
    }
   ],
   "source": [
    "# initialize dictionary to hold lin. reg. models for each element\n",
    "reg = {}\n",
    "for element in elements: \n",
    "\n",
    "    # get IDs of standard reference materials\n",
    "    srm = srm_data.loc[srm_data[\"Analyte\"]==element][\"Sample ID\"].unique() \n",
    "\n",
    "    ## TRAIN \n",
    "    # limit training data to standards for which we have standard reference material info for the element at hand\n",
    "    data_train = xrf_data.loc[(xrf_data[\"qaqc_type\"]==\"standard\") & (xrf_data[\"sample_id\"].isin(srm))]\n",
    "    data_train = data_train.dropna(subset=[element]) # change to true condition statement\n",
    "\n",
    "    x_train = [srm_data.loc[\n",
    "                            (srm_data[\"Sample ID\"]==sample) & \\\n",
    "                            (srm_data[\"Analyte\"]==element)\n",
    "                            ][\"Certified Value\"].values[0] for sample in data_train[\"sample_id\"]]\n",
    "    x_train = np.array(x_train)\n",
    "    y_train = data_train[element].to_numpy()\n",
    "\n",
    "    model = linregress(x_train, y_train) # fit linear regression model\n",
    "\n",
    "    if model.slope != 0: #only use calibration curve if meaningful (i.e., if variance in dep. var. explained by variance in indep. var.)\n",
    "        # invert model so that measured concentration is independent var. and true concentration is dependent var. (i.e., y = m*x + b --> x = (1/m)*y - (b/m))\n",
    "        intercept_inv = -model.intercept / model.slope\n",
    "        slope_inv = model.slope ** -1\n",
    "\n",
    "        ## PREDICT (i.e., calibrate)\n",
    "        data_predict = xrf_data.loc[xrf_data[\"qaqc_type\"]!=\"standard\"]\n",
    "        data_predict = data_predict.dropna(subset=[element])\n",
    "\n",
    "        x_predict = data_predict[element]\n",
    "        x_predict = x_predict.to_numpy()\n",
    "        y_predict = slope_inv * x_predict + intercept_inv\n",
    "\n",
    "        ## Save model results\n",
    "        reg[element] = {} # initialize empty dict to save model results\n",
    "\n",
    "        reg[element][\"model\"]                 = model\n",
    "        reg[element][\"x_train\"]               = x_train\n",
    "        reg[element][\"y_train\"]               = y_train\n",
    "        reg[element][\"score\"]                 = model.rvalue\n",
    "        reg[element][\"y-intercept std error\"] = model.intercept_stderr\n",
    "        reg[element][\"slope_inv\"]             = slope_inv\n",
    "        reg[element][\"intercept_inv\"]         = intercept_inv\n",
    "        reg[element][\"x_predict\"]             = x_predict\n",
    "        reg[element][\"y_predict\"]             = y_predict\n",
    "        \n",
    "        xrf_data.loc[xrf_data[\"qaqc_type\"]!=\"standard\", element] = y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the results of the calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df653cf7ea945c9b008b6784bc75912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Elements', options=('Ag', 'As', 'Ba', 'Bi', 'Ca', 'Cd', 'Câ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dropdown_buttons = {\n",
    "    \"data\": \n",
    "        {\n",
    "            \"name\": \"Elements\", \n",
    "            \"columns\": list(reg.keys())\n",
    "        }\n",
    "    }\n",
    "\n",
    "interactive_linear_regression_calibration_plot(dropdown_buttons, reg, x_axis_label=\"True concentration (ppm)\", y_axis_label=\"Measured concentration (ppm)\", title=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the detection limit for each element. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in elements: \n",
    "    detection_limit = reg[element][\"slope_inv\"] * (reg[element][\"model\"].intercept + 3*reg[element][\"y-intercept std error\"]) + reg[element][\"intercept_inv\"]\n",
    "    reg[element][\"detection_limit\"] = detection_limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply detection limit to dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in reg.keys(): \n",
    "    limit = reg[element][\"detection_limit\"]\n",
    "    xrf_data[element].where(xrf_data[element] >= limit, other=f\"<{limit}\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_duplicates = xrf_data[xrf_data[\"qaqc_type\"]==\"lab duplicate\"]\n",
    "\n",
    "lab_parent_indexes = []\n",
    "\n",
    "for row in lab_duplicates.iterrows(): \n",
    "    lab_dup_id = row[1][\"sample_id\"]\n",
    "    lab_parent_id = lab_dup_id.rstrip('L')\n",
    "\n",
    "    condition = xrf_data[\"sample_id\"] == lab_parent_id\n",
    "    index = xrf_data.index[condition][0]\n",
    "    lab_parent_indexes.append(index)\n",
    "\n",
    "lab_parents = xrf_data.iloc[lab_parent_indexes]\n",
    "\n",
    "lab_pairs = pd.concat([lab_duplicates, lab_parents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 47)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_duplicates = xrf_data[xrf_data[\"qaqc_type\"]==\"field duplicate\"]\n",
    "field_parent_indexes = []\n",
    "\n",
    "for row in field_duplicates.iterrows(): \n",
    "        field_dup_id = row[1][\"sample_id\"]\n",
    "        field_parent_id = field_dup_id.rstrip('F')\n",
    "\n",
    "        condition = xrf_data[\"sample_id\"] == field_parent_id\n",
    "        index = xrf_data.index[condition][0]\n",
    "        field_parent_indexes.append(index)\n",
    "\n",
    "field_parents = xrf_data.iloc[field_parent_indexes]\n",
    "\n",
    "field_pairs = pd.concat([field_duplicates, field_parents])\n",
    "field_parents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_pair_diffs = lab_parents.copy()\n",
    "\n",
    "elements_dup = get_elements(xrf_data.columns.to_list())\n",
    "\n",
    "lab_pair_diffs.loc[:, elements_dup] = lab_pair_diffs.loc[:, elements_dup].apply(pd.to_numeric, errors=\"coerce\")\n",
    "lab_duplicates = lab_duplicates.copy()\n",
    "lab_duplicates.loc[:, elements_dup] = lab_duplicates.loc[:, elements_dup].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "for row in lab_pair_diffs.iterrows():\n",
    "    parent_id = row[1][\"sample_id\"]\n",
    "\n",
    "    parent_entry = row[1].to_frame().transpose()\n",
    "    parent_entry.reset_index(inplace=True)\n",
    "    parent_values = parent_entry.loc[:, elements_dup]\n",
    "\n",
    "    duplicate_entry = lab_duplicates.loc[lab_duplicates[\"sample_id\"]==parent_id + \"L\"]\n",
    "    duplicate_entry.reset_index(inplace=True)\n",
    "    duplicate_values = duplicate_entry.loc[:, elements_dup]\n",
    "    \n",
    "    condition = lab_pair_diffs[\"sample_id\"] == parent_id\n",
    "    lab_pair_diffs.loc[condition, elements_dup] = parent_values.subtract(duplicate_values).values\n",
    "lab_pair_diffs = lab_pair_diffs[[\"sample_id\"] + elements_dup]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_pair_diffs = lab_parents.copy()\n",
    "\n",
    "elements_dup = get_elements(xrf_data.columns.to_list())\n",
    "\n",
    "lab_pair_diffs.loc[:, elements_dup] = lab_pair_diffs.loc[:, elements_dup].apply(pd.to_numeric, errors=\"coerce\")\n",
    "lab_duplicates = lab_duplicates.copy()\n",
    "lab_duplicates.loc[:, elements_dup] = lab_duplicates.loc[:, elements_dup].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "for row in lab_pair_diffs.iterrows():\n",
    "    parent_id = row[1][\"sample_id\"]\n",
    "\n",
    "    parent_entry = row[1].to_frame().transpose()\n",
    "    parent_entry.reset_index(inplace=True)\n",
    "    parent_values = parent_entry.loc[:, elements_dup]\n",
    "\n",
    "    duplicate_entry = lab_duplicates.loc[lab_duplicates[\"sample_id\"]==parent_id + \"L\"]\n",
    "    duplicate_entry.reset_index(inplace=True)\n",
    "    duplicate_values = duplicate_entry.loc[:, elements_dup]\n",
    "    \n",
    "    condition = lab_pair_diffs[\"sample_id\"] == parent_id\n",
    "    print(lab_pair_diffs.loc[condition, elements_dup])\n",
    "    print(parent_values.subtract(duplicate_values))\n",
    "    # print(type(parent_values.subtract(duplicate_values)))\n",
    "    # print(type(lab_pair_diffs.loc[lab_pair_diffs[\"sample_id\"] == parent_id, elements_dup]))\n",
    "\n",
    "# duplicate_values.head()\n",
    "# parent_values.head()\n",
    "# dff = pd.concat([duplicate_values, parent_values])\n",
    "# dff.iloc[1].subtract(dff.iloc[0]).transpose()\n",
    "# duplicate_entry.loc[:, elements_dup].head()\n",
    "# lab_pair_diffs.head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_dev_from_pairs(diffs): \n",
    "    diffs = np.asarray(diffs)\n",
    "    n = diffs.size\n",
    "    \n",
    "    std_dev = np.sqrt(np.sum(np.square(diffs)) / (2 * n))\n",
    "\n",
    "    return std_dev, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s5/y_27w4ms6tj21xxv0z3tg3cw0000gn/T/ipykernel_27408/812834540.py:5: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in double_scalars\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analytical_precision = {}\n",
    "for element in elements_dup: \n",
    "    analytical_precision[element] = dict.fromkeys([\"standard deviation\", \"number of pairs\"])\n",
    "\n",
    "    diffs = lab_pair_diffs[element].dropna()\n",
    "    if len(diffs) >=3: # min. for Dixon's Q Test\n",
    "        diffs = dixon_test(list(diffs), left=True, right=True, confidence_level=95)[0]\n",
    "    std_dev, n = std_dev_from_pairs(diffs)\n",
    "\n",
    "    if np.isnan(std_dev): \n",
    "        std_dev = None\n",
    "\n",
    "    analytical_precision[element][\"standard deviation\"] = std_dev\n",
    "    analytical_precision[element][\"number of pairs\"] = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/interim/analytical_precision.json\", \"w\") as outfile: \n",
    "    json.dump(analytical_precision, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha= 0.05\n",
    "# element = \"Ba\"\n",
    "# normality_test_data = xrf_data.loc[xrf_data[\"qaqc_type\"]!=\"standard\", element]\n",
    "# k, p = stats.normaltest(normality_test_data)\n",
    "# if p < alpha:\n",
    "#     print(\"reject null hypothesis that sample is normall distributed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export data to csv file\n",
    "xrf_data.to_csv('../data/interim/xrf_data_calib.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # xrf_data.to_excel('../data/interim/xrf_data_calib.xlsx', na_rep=\"NaN\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da7f753086fad63f468c7afa1043fdfff877310ce7148e2af11ff3397961b305"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('EPSC-552-Mont-St-Hilaire': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
