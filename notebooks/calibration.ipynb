{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import json\n",
    "from scipy.stats import linregress\n",
    "from statsmodels.stats.weightstats import ztest as ztest\n",
    "\n",
    "# local code\n",
    "from source.interactive_plots import interactive_linear_regression_calibration_plot\n",
    "from source.get_elements      import get_elements\n",
    "from source.outliers          import dixon_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from XRF analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrf_data = pd.read_csv(\"../data/interim/xrf_data_clean.csv\") # load xrf data\n",
    "# drop uncertainty columns \n",
    "xrf_data.drop([column for column in xrf_data.columns if column.endswith(\"+/-\")], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data for standard reference materials and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "srm_data = pd.read_csv(\"../data/interim/standard_reference_material_certified_values.csv\") # load SRM data\n",
    "srm_data.drop([\"1SD\", \"95% Confidence Low\", \"95% Confidence High\"], axis=1, inplace=True) # drop unnecessary columns\n",
    "\n",
    "# clean SRM data\n",
    "for row in srm_data.iterrows(): \n",
    "    row[1][\"Sample ID\"] = row[1][\"Sample ID\"].lower()\n",
    "    row[1][\"Analyte\"] = row[1][\"Analyte\"].split(\",\",1)[0]\n",
    "\n",
    "    # clean certified value\n",
    "    cert_val = row[1][\"Certified Value\"]\n",
    "    if cert_val.startswith(\"<\"): \n",
    "        row[1][\"Certified Value\"] = float(cert_val.lstrip(\"< \")) / 2 # replace BDL with half value\n",
    "    else:\n",
    "        row[1][\"Certified Value\"] = float(row[1][\"Certified Value\"])\n",
    "\n",
    "    # clean units\n",
    "    row[1][\"Units\"] = row[1][\"Units\"].lstrip(\"(\").rstrip(\")\")\n",
    "    if row[1][\"Units\"] == \"wt.%\":  # convert units\n",
    "       row[1][\"Certified Value\"] = row[1][\"Certified Value\"] * 1e4\n",
    "       row[1][\"Units\"] = \"ppm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove standard data for a given element that is unsuitable for calibration\n",
    "\n",
    "Compare the measured concentration of each element for a given standard to the distribution of measured concentrations for that element across all non-standards. If the measured concentration \n",
    "\n",
    "    1.) falls outside the range of the distribution \n",
    "    2.) is more than three standard deviations from the mean \n",
    "\n",
    "that value is set to `NaN` (all values set to `NaN` are filtered out during calibration). \n",
    "\n",
    "Note: I should consider conducting a test to determine if each distribution is normally or log-normally distributed; if the latter, I should log-transform the data and then calculate the z-score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score(data, value): \n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "\n",
    "    return (value - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_standards_data = xrf_data.loc[xrf_data[\"qaqc_type\"]!=\"standard\"]\n",
    "\n",
    "outlier_stddev_cutoff = 5\n",
    "\n",
    "for standard in xrf_data.loc[xrf_data[\"qaqc_type\"]==\"standard\", \"sample_id\"].unique(): \n",
    "    for date in xrf_data.loc[xrf_data[\"sample_id\"]==standard, \"date\"].unique():\n",
    "        for element in get_elements(xrf_data.columns.to_list()): \n",
    "            standard_date_element = xrf_data.loc[(xrf_data[\"sample_id\"]==standard) & (xrf_data[\"date\"]==date)][element].values\n",
    "\n",
    "            if (non_standards_data[element] > standard_date_element[0]).all() | \\\n",
    "               (non_standards_data[element] < standard_date_element[0]).all(): \n",
    "                \n",
    "                # print(non_standards_data[element].to_numpy())\n",
    "                z = z_score(non_standards_data[element], standard_date_element[0])\n",
    "                if abs(z) > outlier_stddev_cutoff: \n",
    "                    # print(standard + \", \" + date + \", \" + element + \", \" + str(standard_date_element[0]) + \", \" + str(z))\n",
    "\n",
    "                    xrf_data.loc[(xrf_data[\"sample_id\"]==standard) & (xrf_data[\"date\"]==date), element] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the elements analyzed by the XRF and for which concentrations are reported for one or more standard reference materials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = get_elements(\n",
    "                        list(\n",
    "                            set(srm_data[\"Analyte\"].unique()) & \\\n",
    "                            set(xrf_data.columns.to_list())\n",
    "                            )\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a linear regression model for each element in order to predict the true concentration from the measured concentration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertcollar/miniconda3/envs/EPSC-552-Mont-St-Hilaire/lib/python3.9/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in double_scalars\n",
      "\n",
      "/Users/robertcollar/miniconda3/envs/EPSC-552-Mont-St-Hilaire/lib/python3.9/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in double_scalars\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize dictionary to hold lin. reg. models for each element\n",
    "reg = {}\n",
    "for element in elements: \n",
    "\n",
    "    # get IDs of standard reference materials\n",
    "    srm = srm_data.loc[srm_data[\"Analyte\"]==element][\"Sample ID\"].unique() \n",
    "\n",
    "    ## TRAIN \n",
    "    # limit training data to standards for which we have standard reference material info for the element at hand\n",
    "    data_train = xrf_data.loc[(xrf_data[\"qaqc_type\"]==\"standard\") & (xrf_data[\"sample_id\"].isin(srm))]\n",
    "    data_train = data_train.dropna(subset=[element]) # change to true condition statement\n",
    "\n",
    "    x_train = [srm_data.loc[\n",
    "                            (srm_data[\"Sample ID\"]==sample) & \\\n",
    "                            (srm_data[\"Analyte\"]==element)\n",
    "                            ][\"Certified Value\"].values[0] for sample in data_train[\"sample_id\"]]\n",
    "    x_train = np.array(x_train)\n",
    "    y_train = data_train[element].to_numpy()\n",
    "\n",
    "    model = linregress(x_train, y_train) # fit linear regression model\n",
    "\n",
    "    if model.slope != 0: #only use calibration curve if meaningful (i.e., if variance in dep. var. explained by variance in indep. var.)\n",
    "        # invert model so that measured concentration is independent var. and true concentration is dependent var. (i.e., y = m*x + b --> x = (1/m)*y - (b/m))\n",
    "        intercept_inv = -model.intercept / model.slope\n",
    "        slope_inv = model.slope ** -1\n",
    "\n",
    "        ## PREDICT (i.e., calibrate)\n",
    "        data_predict = xrf_data.loc[xrf_data[\"qaqc_type\"]!=\"standard\"]\n",
    "        data_predict = data_predict.dropna(subset=[element])\n",
    "\n",
    "        x_predict = data_predict[element]\n",
    "        x_predict = x_predict.to_numpy()\n",
    "        y_predict = slope_inv * x_predict + intercept_inv\n",
    "\n",
    "        ## Save model results\n",
    "        reg[element] = {} # initialize empty dict to save model results\n",
    "\n",
    "        reg[element][\"model\"]                 = model\n",
    "        reg[element][\"x_train\"]               = x_train\n",
    "        reg[element][\"y_train\"]               = y_train\n",
    "        reg[element][\"rvalue\"]                 = model.rvalue\n",
    "        reg[element][\"y-intercept std error\"] = model.intercept_stderr\n",
    "        reg[element][\"slope_inv\"]             = slope_inv\n",
    "        reg[element][\"intercept_inv\"]         = intercept_inv\n",
    "        reg[element][\"x_predict\"]             = x_predict\n",
    "        reg[element][\"y_predict\"]             = y_predict\n",
    "        \n",
    "        xrf_data.loc[xrf_data[\"qaqc_type\"]!=\"standard\", element] = y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the results of the calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3fc7b9b26ab4d18ae63ffc1bcf38a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Elements', options=('Ag', 'As', 'Ba', 'Bi', 'Ca', 'Cd', 'Câ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dropdown_buttons = {\n",
    "    \"data\": \n",
    "        {\n",
    "            \"name\": \"Elements\", \n",
    "            \"columns\": list(reg.keys())\n",
    "        }\n",
    "    }\n",
    "\n",
    "interactive_linear_regression_calibration_plot(dropdown_buttons, reg, x_axis_label=\"True concentration (ppm)\", y_axis_label=\"Measured concentration (ppm)\", title=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the detection limit for each element. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in elements: \n",
    "    detection_limit = reg[element][\"slope_inv\"] * (reg[element][\"model\"].intercept + 3*reg[element][\"y-intercept std error\"]) + reg[element][\"intercept_inv\"]\n",
    "    reg[element][\"detection_limit\"] = detection_limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply detection limit to dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in reg.keys(): \n",
    "    limit = reg[element][\"detection_limit\"]\n",
    "    xrf_data[element].where(xrf_data[element] >= limit, other=f\"<{limit}\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_duplicates = xrf_data[xrf_data[\"qaqc_type\"]==\"lab duplicate\"]\n",
    "\n",
    "lab_parent_indexes = []\n",
    "\n",
    "for row in lab_duplicates.iterrows(): \n",
    "    lab_dup_id = row[1][\"sample_id\"]\n",
    "    lab_parent_id = lab_dup_id.rstrip('L')\n",
    "\n",
    "    condition = xrf_data[\"sample_id\"] == lab_parent_id\n",
    "    index = xrf_data.index[condition][0]\n",
    "    lab_parent_indexes.append(index)\n",
    "\n",
    "lab_parents = xrf_data.iloc[lab_parent_indexes]\n",
    "\n",
    "lab_pairs = pd.concat([lab_duplicates, lab_parents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def half_detection_limit(x): \n",
    "    if isinstance(x, str): \n",
    "        if x.startswith(\"<\"): \n",
    "            x = float(x.lstrip(\"< \")) / 2 # replace BDL with half value\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_pair_diffs = lab_parents.copy()\n",
    "lab_duplicates = lab_duplicates.copy()\n",
    "\n",
    "elements_dup = get_elements(xrf_data.columns.to_list())\n",
    "\n",
    "for row in lab_pair_diffs.iterrows():\n",
    "    parent_id = row[1][\"sample_id\"]\n",
    "    duplicate_id = parent_id + \"L\"\n",
    "\n",
    "    parent_entry = row[1].to_frame().transpose()\n",
    "    parent_entry.reset_index(inplace=True)\n",
    "    parent_values = parent_entry.loc[:, elements_dup]\n",
    "\n",
    "    duplicate_entry = lab_duplicates.loc[lab_duplicates[\"sample_id\"]==duplicate_id]\n",
    "    duplicate_entry.reset_index(inplace=True)\n",
    "    duplicate_values = duplicate_entry.loc[:, elements_dup]\n",
    "    \n",
    "    condition = lab_pair_diffs[\"sample_id\"] == parent_id\n",
    "    lab_pair_diffs.loc[condition, elements_dup] = parent_values.apply(pd.to_numeric, errors=\"coerce\").\\\n",
    "                                                  subtract(duplicate_values.apply(pd.to_numeric, errors=\"coerce\")).values\n",
    "\n",
    "    parent_values = parent_values.applymap(half_detection_limit)\n",
    "    duplicate_values = duplicate_values.applymap(half_detection_limit)\n",
    "\n",
    "    pair_values = pd.concat((parent_values, duplicate_values))\n",
    "\n",
    "    condition = xrf_data[\"sample_id\"] == parent_id\n",
    "    xrf_data.loc[condition, elements_dup] = pair_values.mean().values\n",
    "    \n",
    "    condition = xrf_data[\"sample_id\"] == duplicate_id\n",
    "    xrf_data.drop(xrf_data[condition].index, inplace=True)\n",
    "xrf_data.reset_index(inplace=True)\n",
    "\n",
    "lab_pair_diffs = lab_pair_diffs[[\"sample_id\"] + elements_dup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_dev_from_pairs(diffs): \n",
    "    diffs = np.asarray(diffs)\n",
    "    n = diffs.size\n",
    "    \n",
    "    std_dev = np.sqrt(np.sum(np.square(diffs)) / (2 * n))\n",
    "\n",
    "    return std_dev, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_std_dev(element, data, std_dev): \n",
    "    data = data.applymap(half_detection_limit)\n",
    "    mean = data.loc[xrf_data[\"qaqc_type\"] != \"standard\", element].mean()\n",
    "    rsd = 100 * std_dev / mean\n",
    "\n",
    "    return rsd, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytical_precision = {}\n",
    "xrf_data_bdl_replaced = xrf_data.applymap(half_detection_limit)\n",
    "for element in elements_dup: \n",
    "    diffs = lab_pair_diffs[element]\n",
    "\n",
    "    if False in list(diffs.isnull()): # ensure that there are >= 1 valid data values\n",
    "        diffs.dropna(inplace=True)\n",
    "\n",
    "        if (len(diffs) >=3): # min. for Dixon's Q Test\n",
    "            diffs = dixon_test(list(diffs), left=True, right=True, confidence_level=95)[0]\n",
    "        std_dev, n = std_dev_from_pairs(diffs)\n",
    "        \n",
    "        mean = xrf_data_bdl_replaced.loc[xrf_data_bdl_replaced[\"qaqc_type\"] != \"standard\", element].mean()\n",
    "        rsd = 100 * std_dev / mean\n",
    "\n",
    "        analytical_precision[element] = {}\n",
    "        analytical_precision[element][\"standard deviation\"] = std_dev\n",
    "        analytical_precision[element][\"number of pairs\"] = n\n",
    "        analytical_precision[element][\"rsd\"] = rsd\n",
    "        analytical_precision[element][\"mean\"] = mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/interim/analytical_precision.json\", \"w\") as outfile: \n",
    "    json.dump(analytical_precision, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field heterogeneity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get field duplicate-parent pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_duplicates = xrf_data[xrf_data[\"qaqc_type\"]==\"field duplicate\"]\n",
    "field_parent_indexes = []\n",
    "\n",
    "for row in field_duplicates.iterrows(): \n",
    "        field_dup_id = row[1][\"sample_id\"]\n",
    "        field_parent_id = field_dup_id.rstrip('F')\n",
    "\n",
    "        condition = xrf_data[\"sample_id\"] == field_parent_id\n",
    "        index = xrf_data.index[condition][0]\n",
    "        field_parent_indexes.append(index)\n",
    "\n",
    "field_parents = xrf_data.iloc[field_parent_indexes]\n",
    "\n",
    "field_pairs = pd.concat([field_duplicates, field_parents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate difference between each duplicate-parent pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements_dup = get_elements(xrf_data.columns.to_list())\n",
    "\n",
    "field_pair_diffs = field_parents.copy()\n",
    "field_pair_diffs.loc[:, elements_dup] = field_pair_diffs.loc[:, elements_dup].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "field_duplicates = field_duplicates.copy()\n",
    "field_duplicates.loc[:, elements_dup] = field_duplicates.loc[:, elements_dup].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "for row in field_pair_diffs.iterrows():\n",
    "    parent_id = row[1][\"sample_id\"]\n",
    "    duplicate_id = parent_id + \"F\"\n",
    "\n",
    "    parent_entry = row[1].to_frame().transpose()\n",
    "    parent_entry.reset_index(inplace=True)\n",
    "    parent_values = parent_entry.loc[:, elements_dup]\n",
    "\n",
    "    duplicate_entry = field_duplicates.loc[field_duplicates[\"sample_id\"]==duplicate_id]\n",
    "    duplicate_entry.reset_index(inplace=True)\n",
    "    duplicate_values = duplicate_entry.loc[:, elements_dup]\n",
    "    \n",
    "    condition = field_pair_diffs[\"sample_id\"] == parent_id\n",
    "    field_pair_diffs.loc[condition, elements_dup] = parent_values.subtract(duplicate_values).values\n",
    "\n",
    "\n",
    "field_pair_diffs = field_pair_diffs[[\"sample_id\"] + elements_dup]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate field heterogeneity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_heterogeneity = {}\n",
    "for element in elements_dup: \n",
    "    diffs = field_pair_diffs[element]\n",
    "    \n",
    "    if False in list(diffs.isnull()): # ensure that there are >= 1 valid data values\n",
    "        diffs.dropna(inplace=True) \n",
    "\n",
    "        if len(diffs) >=3: # min. for Dixon's Q Test \n",
    "            diffs = dixon_test(list(diffs), left=True, right=True, confidence_level=95)[0]\n",
    "        std_dev, n = std_dev_from_pairs(diffs)\n",
    "        mean = xrf_data_bdl_replaced.loc[xrf_data_bdl_replaced[\"qaqc_type\"] != \"standard\", element].mean()\n",
    "\n",
    "\n",
    "        if element in analytical_precision.keys(): # analytical precision must be calculated to determine field heterogeneity\n",
    "            field_heterogeneity[element] = {}\n",
    "            field_heterogeneity[element][\"heterogeneity\"] = std_dev - analytical_precision[element][\"standard deviation\"]\n",
    "            field_heterogeneity[element][\"relative heterogeneity\"] = 100 * field_heterogeneity[element][\"heterogeneity\"] / mean\n",
    "            field_heterogeneity[element][\"number of pairs\"] = n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save field heterogeneity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/interim/field_heterogeneity.json\", \"w\") as outfile: \n",
    "    json.dump(field_heterogeneity, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha= 0.05\n",
    "# element = \"Ba\"\n",
    "# normality_test_data = xrf_data.loc[xrf_data[\"qaqc_type\"]!=\"standard\", element]\n",
    "# k, p = stats.normaltest(normality_test_data)\n",
    "# if p < alpha:\n",
    "#     print(\"reject null hypothesis that sample is normall distributed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export data to csv file\n",
    "xrf_data.to_csv('../data/interim/xrf_data_calib.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # xrf_data.to_excel('../data/interim/xrf_data_calib.xlsx', na_rep=\"NaN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in reg.keys(): \n",
    "    num_bdl = 0\n",
    "    for value in xrf_data[element]:\n",
    "        if isinstance(value, str):\n",
    "            if value.startswith(\"<\"):\n",
    "                num_bdl+=1\n",
    "    prop_bdl = num_bdl/xrf_data.shape[0]\n",
    "    reg[element][\"proportion_bdl\"] = prop_bdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_evaluation = dict.fromkeys(get_elements(xrf_data.columns))\n",
    "for element in dataset_evaluation.keys(): \n",
    "    dataset_evaluation[element] = {\"calibration\":          {}, \\\n",
    "                                   \"analytical precision\": {}, \\\n",
    "                                   \"field heterogeneity\":  {}}\n",
    "\n",
    "for element in reg.keys(): \n",
    "    dataset_evaluation[element][\"calibration\"] = {\"Pearson correlation coefficient\": round(reg[element][\"rvalue\"], 3), \\\n",
    "                                                  \"R-squared\":                       round(reg[element][\"rvalue\"]**2, 3), \\\n",
    "                                                  \"proportion_bdl\":                  round(reg[element][\"proportion_bdl\"], 1)}\n",
    "\n",
    "for element in analytical_precision.keys(): \n",
    "    dataset_evaluation[element][\"analytical precision\"] = {\"standard deviation\": round(analytical_precision[element][\"standard deviation\"], 2), \\\n",
    "                                                           \"rsd\":                round(analytical_precision[element][\"rsd\"],                2), \\\n",
    "                                                           \"number of pairs\":    round(analytical_precision[element][\"number of pairs\"],    2)}\n",
    "\n",
    "for element in field_heterogeneity.keys(): \n",
    "    dataset_evaluation[element][\"field heterogeneity\"] = {\"heterogeneity\":   round(field_heterogeneity[element][\"heterogeneity\"],   2), \\\n",
    "                                                          \"number of pairs\": round(field_heterogeneity[element][\"number of pairs\"], 2)}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "discard = {}\n",
    "usable_dataset_evaluation = dataset_evaluation.copy()\n",
    "for element in dataset_evaluation.keys(): \n",
    "    if dataset_evaluation[element][\"calibration\"] == {}:\n",
    "        discard[element] = \"srm values missing\"\n",
    "    \n",
    "    elif dataset_evaluation[element][\"calibration\"][\"Pearson correlation coefficient\"] <= 0: \n",
    "        discard[element] = \"zero or negative correlation between reported and measured values of srms\"\n",
    "\n",
    "    elif dataset_evaluation[element][\"calibration\"][\"R-squared\"] <= 0.3:\n",
    "        discard[element] = \"proportion of variation in srm measured values explained by reported values is low\"\n",
    "\n",
    "    elif dataset_evaluation[element][\"analytical precision\"] == {}: \n",
    "        discard[element] = \"insufficient data to calculate analytical precision\"\n",
    "        \n",
    "    elif dataset_evaluation[element][\"analytical precision\"][\"number of pairs\"] < 9:\n",
    "        discard[element] = \"insufficient usable data to calculate analytical precision\"\n",
    "    \n",
    "    elif dataset_evaluation[element][\"analytical precision\"][\"rsd\"] > 30:\n",
    "        discard[element] = \"RSD is too high\"\n",
    "    \n",
    "    elif dataset_evaluation[element][\"calibration\"][\"proportion_bdl\"] > 1/3: \n",
    "        discard[element] = \"proportion of values below the detection limit is too high\"\n",
    "\n",
    "for element in discard.keys():\n",
    "    usable_dataset_evaluation.pop(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'calibration': {'Pearson correlation coefficient': 0.55,\n",
       "  'R-squared': 0.303,\n",
       "  'proportion_bdl': 0.4},\n",
       " 'analytical precision': {'standard deviation': 0.94,\n",
       "  'rsd': 18.9,\n",
       "  'number of pairs': 9},\n",
       " 'field heterogeneity': {'heterogeneity': 0.8, 'number of pairs': 4}}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_evaluation[\"Mo\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean analysis-ready dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ag': 'proportion of variation in srm measured values explained by reported values is low',\n",
       " 'Au': 'srm values missing',\n",
       " 'Bi': 'proportion of variation in srm measured values explained by reported values is low',\n",
       " 'Cd': 'zero or negative correlation between reported and measured values of srms',\n",
       " 'Ce': 'zero or negative correlation between reported and measured values of srms',\n",
       " 'Cl': 'srm values missing',\n",
       " 'Co': 'insufficient usable data to calculate analytical precision',\n",
       " 'Cr': 'insufficient usable data to calculate analytical precision',\n",
       " 'Hg': 'srm values missing',\n",
       " 'Mo': 'proportion of values below the detection limit is too high',\n",
       " 'Ni': 'zero or negative correlation between reported and measured values of srms',\n",
       " 'Pd': 'srm values missing',\n",
       " 'Pr': 'zero or negative correlation between reported and measured values of srms',\n",
       " 'Pt': 'srm values missing',\n",
       " 'Rh': 'srm values missing',\n",
       " 'S': 'proportion of variation in srm measured values explained by reported values is low',\n",
       " 'Sb': 'proportion of variation in srm measured values explained by reported values is low',\n",
       " 'Se': 'zero or negative correlation between reported and measured values of srms',\n",
       " 'Sm': 'zero or negative correlation between reported and measured values of srms',\n",
       " 'Sn': 'proportion of variation in srm measured values explained by reported values is low',\n",
       " 'U': 'proportion of variation in srm measured values explained by reported values is low',\n",
       " 'Zr': 'proportion of variation in srm measured values explained by reported values is low'}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_analysis_ready = xrf_data.copy()\n",
    "# remove standards\n",
    "data_analysis_ready.drop(data_analysis_ready[data_analysis_ready[\"qaqc_type\"]==\"standard\"].index, inplace=True)\n",
    "# remove sediment samples\n",
    "data_analysis_ready.drop(data_analysis_ready[data_analysis_ready[\"sample_type\"]==\"sediment\"].index, inplace=True)\n",
    "# remove unsuitable elements\n",
    "data_analysis_ready.drop(discard.keys(), axis=1, inplace=True)\n",
    "# remove unnecessary columns\n",
    "data_analysis_ready.drop([\"index\", \"Unnamed: 0\", \"date\", \"group\", \"sample_type\", \"qaqc_type\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_analysis_ready = data_analysis_ready.applymap(half_detection_limit)\n",
    "\n",
    "for element in get_elements(data_analysis_ready.keys()): \n",
    "    precision = dataset_evaluation[element][\"analytical precision\"][\"standard deviation\"]\n",
    "    precision_oom = np.floor(np.log10(precision)) - 1\n",
    "    data_analysis_ready[element] = np.round(data_analysis_ready[element] / 10**precision_oom) * 10**precision_oom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export data to csv file\n",
    "data_analysis_ready.to_csv('../data/interim/data_analysis_ready.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sample_id', 'As', 'Ba', 'Ca', 'Cu', 'Fe', 'K', 'Mn', 'P', 'Pb', 'Rb',\n",
       "       'Sr', 'Ti', 'V', 'Zn', 'Nb', 'Y', 'La', 'Nd', 'comments'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_analysis_ready.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da7f753086fad63f468c7afa1043fdfff877310ce7148e2af11ff3397961b305"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('EPSC-552-Mont-St-Hilaire': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
