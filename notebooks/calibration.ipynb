{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import json\n",
    "from scipy.stats import linregress\n",
    "from statsmodels.stats.weightstats import ztest as ztest\n",
    "\n",
    "# local code\n",
    "from source.interactive_plots import interactive_linear_regression_calibration_plot\n",
    "from source.get_elements      import get_elements\n",
    "from source.outliers          import dixon_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from XRF analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrf_data = pd.read_csv(\"../data/interim/xrf_data_clean.csv\") # load xrf data\n",
    "# drop uncertainty columns \n",
    "xrf_data.drop([column for column in xrf_data.columns if column.endswith(\"+/-\")], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data for standard reference materials and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "srm_data = pd.read_csv(\"../data/interim/standard_reference_material_certified_values.csv\") # load SRM data\n",
    "srm_data.drop([\"1SD\", \"95% Confidence Low\", \"95% Confidence High\"], axis=1, inplace=True) # drop unnecessary columns\n",
    "\n",
    "# clean SRM data\n",
    "for row in srm_data.iterrows(): \n",
    "    row[1][\"Sample ID\"] = row[1][\"Sample ID\"].lower()\n",
    "    row[1][\"Analyte\"] = row[1][\"Analyte\"].split(\",\",1)[0]\n",
    "\n",
    "    # clean certified value\n",
    "    cert_val = row[1][\"Certified Value\"]\n",
    "    if cert_val.startswith(\"<\"): \n",
    "        row[1][\"Certified Value\"] = float(cert_val.lstrip(\"< \")) / 2 # replace BDL with half value\n",
    "    else:\n",
    "        row[1][\"Certified Value\"] = float(row[1][\"Certified Value\"])\n",
    "\n",
    "    # clean units\n",
    "    row[1][\"Units\"] = row[1][\"Units\"].lstrip(\"(\").rstrip(\")\")\n",
    "    if row[1][\"Units\"] == \"wt.%\":  # convert units\n",
    "       row[1][\"Certified Value\"] = row[1][\"Certified Value\"] * 1e4\n",
    "       row[1][\"Units\"] = \"ppm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove standard data for a given element that is unsuitable for calibration\n",
    "\n",
    "Compare the measured concentration of each element for a given standard to the distribution of measured concentrations for that element across all non-standards. If the measured concentration \n",
    "\n",
    "    1.) falls outside the range of the distribution \n",
    "    2.) is more than three standard deviations from the mean \n",
    "\n",
    "that value is set to `NaN` (all values set to `NaN` are filtered out during calibration). \n",
    "\n",
    "Note: I should consider conducting a test to determine if each distribution is normally or log-normally distributed; if the latter, I should log-transform the data and then calculate the z-score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score(data, value): \n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "\n",
    "    return (value - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_standards_data = xrf_data.loc[xrf_data[\"qaqc_type\"]!=\"standard\"]\n",
    "\n",
    "outlier_stddev_cutoff = 5\n",
    "\n",
    "for standard in xrf_data.loc[xrf_data[\"qaqc_type\"]==\"standard\", \"sample_id\"].unique(): \n",
    "    for date in xrf_data.loc[xrf_data[\"sample_id\"]==standard, \"date\"].unique():\n",
    "        for element in get_elements(xrf_data.columns.to_list()): \n",
    "            standard_date_element = xrf_data.loc[(xrf_data[\"sample_id\"]==standard) & (xrf_data[\"date\"]==date)][element].values\n",
    "\n",
    "            if (non_standards_data[element] > standard_date_element[0]).all() | \\\n",
    "               (non_standards_data[element] < standard_date_element[0]).all(): \n",
    "                \n",
    "                # print(non_standards_data[element].to_numpy())\n",
    "                z = z_score(non_standards_data[element], standard_date_element[0])\n",
    "                if abs(z) > outlier_stddev_cutoff: \n",
    "                    # print(standard + \", \" + date + \", \" + element + \", \" + str(standard_date_element[0]) + \", \" + str(z))\n",
    "\n",
    "                    xrf_data.loc[(xrf_data[\"sample_id\"]==standard) & (xrf_data[\"date\"]==date), element] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38    2.0\n",
       "Name: Mo, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xrf_data[xrf_data[\"sample_id\"]==\"TE2-008\"][\"Mo\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the elements analyzed by the XRF and for which concentrations are reported for one or more standard reference materials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = get_elements(\n",
    "                        list(\n",
    "                            set(srm_data[\"Analyte\"].unique()) & \\\n",
    "                            set(xrf_data.columns.to_list())\n",
    "                            )\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a linear regression model for each element in order to predict the true concentration from the measured concentration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertcollar/miniconda3/envs/EPSC-552-Mont-St-Hilaire/lib/python3.9/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  slope = ssxym / ssxm\n",
      "/Users/robertcollar/miniconda3/envs/EPSC-552-Mont-St-Hilaire/lib/python3.9/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  slope_stderr = np.sqrt((1 - r**2) * ssym / ssxm / df)\n"
     ]
    }
   ],
   "source": [
    "# initialize dictionary to hold lin. reg. models for each element\n",
    "reg = {}\n",
    "for element in elements: \n",
    "\n",
    "    # get IDs of standard reference materials\n",
    "    srm = srm_data.loc[srm_data[\"Analyte\"]==element][\"Sample ID\"].unique() \n",
    "\n",
    "    ## TRAIN \n",
    "    # limit training data to standards for which we have standard reference material info for the element at hand\n",
    "    data_train = xrf_data.loc[(xrf_data[\"qaqc_type\"]==\"standard\") & (xrf_data[\"sample_id\"].isin(srm))]\n",
    "    data_train = data_train.dropna(subset=[element]) # change to true condition statement\n",
    "\n",
    "    x_train = [srm_data.loc[\n",
    "                            (srm_data[\"Sample ID\"]==sample) & \\\n",
    "                            (srm_data[\"Analyte\"]==element)\n",
    "                            ][\"Certified Value\"].values[0] for sample in data_train[\"sample_id\"]]\n",
    "    x_train = np.array(x_train)\n",
    "    y_train = data_train[element].to_numpy()\n",
    "\n",
    "    model = linregress(x_train, y_train) # fit linear regression model\n",
    "\n",
    "    if model.slope != 0: #only use calibration curve if meaningful (i.e., if variance in dep. var. explained by variance in indep. var.)\n",
    "        # invert model so that measured concentration is independent var. and true concentration is dependent var. (i.e., y = m*x + b --> x = (1/m)*y - (b/m))\n",
    "        intercept_inv = -model.intercept / model.slope\n",
    "        slope_inv = model.slope ** -1\n",
    "\n",
    "        ## PREDICT (i.e., calibrate)\n",
    "        data_predict = xrf_data.loc[xrf_data[\"qaqc_type\"]!=\"standard\"]\n",
    "        data_predict = data_predict.dropna(subset=[element])\n",
    "\n",
    "        x_predict = data_predict[element]\n",
    "        x_predict = x_predict.to_numpy()\n",
    "        y_predict = slope_inv * x_predict + intercept_inv\n",
    "\n",
    "        ## Save model results\n",
    "        reg[element] = {} # initialize empty dict to save model results\n",
    "\n",
    "        reg[element][\"model\"]                 = model\n",
    "        reg[element][\"x_train\"]               = x_train\n",
    "        reg[element][\"y_train\"]               = y_train\n",
    "        reg[element][\"score\"]                 = model.rvalue\n",
    "        reg[element][\"y-intercept std error\"] = model.intercept_stderr\n",
    "        reg[element][\"slope_inv\"]             = slope_inv\n",
    "        reg[element][\"intercept_inv\"]         = intercept_inv\n",
    "        reg[element][\"x_predict\"]             = x_predict\n",
    "        reg[element][\"y_predict\"]             = y_predict\n",
    "        \n",
    "        xrf_data.loc[xrf_data[\"qaqc_type\"]!=\"standard\", element] = y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the results of the calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38    1.418657\n",
       "Name: Mo, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xrf_data[xrf_data[\"sample_id\"]==\"TE2-008\"][\"Mo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99995f6abca640f7bf981788a1d96a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Elements', options=('Ag', 'As', 'Ba', 'Bi', 'Ca', 'Cd', 'C…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dropdown_buttons = {\n",
    "    \"data\": \n",
    "        {\n",
    "            \"name\": \"Elements\", \n",
    "            \"columns\": list(reg.keys())\n",
    "        }\n",
    "    }\n",
    "\n",
    "interactive_linear_regression_calibration_plot(dropdown_buttons, reg, x_axis_label=\"True concentration (ppm)\", y_axis_label=\"Measured concentration (ppm)\", title=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the detection limit for each element. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in elements: \n",
    "    detection_limit = reg[element][\"slope_inv\"] * (reg[element][\"model\"].intercept + 3*reg[element][\"y-intercept std error\"]) + reg[element][\"intercept_inv\"]\n",
    "    reg[element][\"detection_limit\"] = detection_limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply detection limit to dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in reg.keys(): \n",
    "    limit = reg[element][\"detection_limit\"]\n",
    "    xrf_data[element].where(xrf_data[element] >= limit, other=f\"<{limit}\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38    <2.4797694423678664\n",
       "Name: Mo, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xrf_data[xrf_data[\"sample_id\"]==\"TE2-008\"][\"Mo\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_duplicates = xrf_data[xrf_data[\"qaqc_type\"]==\"lab duplicate\"]\n",
    "\n",
    "lab_parent_indexes = []\n",
    "\n",
    "for row in lab_duplicates.iterrows(): \n",
    "    lab_dup_id = row[1][\"sample_id\"]\n",
    "    lab_parent_id = lab_dup_id.rstrip('L')\n",
    "\n",
    "    condition = xrf_data[\"sample_id\"] == lab_parent_id\n",
    "    index = xrf_data.index[condition][0]\n",
    "    lab_parent_indexes.append(index)\n",
    "\n",
    "lab_parents = xrf_data.iloc[lab_parent_indexes]\n",
    "\n",
    "lab_pairs = pd.concat([lab_duplicates, lab_parents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def half_detection_limit(x): \n",
    "    if isinstance(x, str): \n",
    "        if x.startswith(\"<\"): \n",
    "            x = float(x.lstrip(\"< \")) / 2 # replace BDL with half value\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38    <2.4797694423678664\n",
       "Name: Mo, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xrf_data[xrf_data[\"sample_id\"]==\"TE2-008\"][\"Mo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48    <2.4797694423678664\n",
       "Name: Mo, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab_pairs[lab_pairs[\"sample_id\"]==\"TE2-008L\"][\"Mo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38    <2.4797694423678664\n",
       "Name: Mo, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab_parents[lab_parents[\"sample_id\"]==\"TE2-008\"][\"Mo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_pair_diffs = lab_parents.copy()\n",
    "lab_duplicates = lab_duplicates.copy()\n",
    "\n",
    "elements_dup = get_elements(xrf_data.columns.to_list())\n",
    "\n",
    "for row in lab_pair_diffs.iterrows():\n",
    "    parent_id = row[1][\"sample_id\"]\n",
    "    duplicate_id = parent_id + \"L\"\n",
    "\n",
    "    parent_entry = row[1].to_frame().transpose()\n",
    "    parent_entry.reset_index(inplace=True)\n",
    "    parent_values = parent_entry.loc[:, elements_dup]\n",
    "\n",
    "    duplicate_entry = lab_duplicates.loc[lab_duplicates[\"sample_id\"]==duplicate_id]\n",
    "    duplicate_entry.reset_index(inplace=True)\n",
    "    duplicate_values = duplicate_entry.loc[:, elements_dup]\n",
    "    \n",
    "    condition = lab_pair_diffs[\"sample_id\"] == parent_id\n",
    "    lab_pair_diffs.loc[condition, elements_dup] = parent_values.apply(pd.to_numeric, errors=\"coerce\").\\\n",
    "                                                  subtract(duplicate_values.apply(pd.to_numeric, errors=\"coerce\")).values\n",
    "\n",
    "    parent_values = parent_values.applymap(half_detection_limit)\n",
    "    duplicate_values = duplicate_values.applymap(half_detection_limit)\n",
    "\n",
    "    pair_values = pd.concat((parent_values, duplicate_values))\n",
    "\n",
    "    condition = xrf_data[\"sample_id\"] == parent_id\n",
    "    xrf_data.loc[condition, elements_dup] = pair_values.mean().values\n",
    "    \n",
    "    condition = xrf_data[\"sample_id\"] == duplicate_id\n",
    "    xrf_data.drop(xrf_data[condition].index, inplace=True)\n",
    "xrf_data.reset_index(inplace=True)\n",
    "\n",
    "lab_pair_diffs = lab_pair_diffs[[\"sample_id\"] + elements_dup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38    NaN\n",
       "Name: Mo, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab_pair_diffs[lab_pair_diffs[\"sample_id\"]==\"TE2-008\"][\"Mo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_dev_from_pairs(diffs): \n",
    "    diffs = np.asarray(diffs)\n",
    "    n = diffs.size\n",
    "    \n",
    "    std_dev = np.sqrt(np.sum(np.square(diffs)) / (2 * n))\n",
    "\n",
    "    return std_dev, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_std_dev(element, data, std_dev): \n",
    "    data = data.applymap(half_detection_limit)\n",
    "    mean = data.loc[xrf_data[\"qaqc_type\"] != \"standard\", element].mean()\n",
    "    rsd = 100 * std_dev / mean\n",
    "\n",
    "    return rsd, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19             NaN\n",
       "30       66.114723\n",
       "38     1168.026767\n",
       "74      330.573613\n",
       "72     1079.873804\n",
       "62             NaN\n",
       "57      396.688336\n",
       "67     3327.774375\n",
       "102     374.650095\n",
       "97     2380.130016\n",
       "92      352.611854\n",
       "87    -1454.523899\n",
       "107     462.803059\n",
       "120   -2336.053534\n",
       "123    -132.229445\n",
       "128            NaN\n",
       "140   -1652.868067\n",
       "145            NaN\n",
       "150    -352.611854\n",
       "153    -374.650095\n",
       "190            NaN\n",
       "139    -154.267686\n",
       "180     -22.038241\n",
       "175    -617.070745\n",
       "170    -550.956022\n",
       "204     264.458891\n",
       "219    -374.650095\n",
       "Name: Zr, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab_pair_diffs[element]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytical_precision = {}\n",
    "xrf_data_bdl_replaced = xrf_data.applymap(half_detection_limit)\n",
    "for element in elements_dup: \n",
    "    diffs = lab_pair_diffs[element]\n",
    "\n",
    "    if False in list(diffs.isnull()): # ensure that there are >= 1 valid data values\n",
    "        diffs.dropna(inplace=True)\n",
    "\n",
    "        if (len(diffs) >=3): # min. for Dixon's Q Test\n",
    "            diffs = dixon_test(list(diffs), left=True, right=True, confidence_level=95)[0]\n",
    "        std_dev, n = std_dev_from_pairs(diffs)\n",
    "        \n",
    "        mean = xrf_data_bdl_replaced.loc[xrf_data_bdl_replaced[\"qaqc_type\"] != \"standard\", element].mean()\n",
    "        rsd = 100 * std_dev / mean\n",
    "\n",
    "        analytical_precision[element] = {}\n",
    "        analytical_precision[element][\"standard deviation\"] = std_dev\n",
    "        analytical_precision[element][\"number of pairs\"] = n\n",
    "        analytical_precision[element][\"rsd\"] = rsd\n",
    "        analytical_precision[element][\"mean\"] = mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/interim/analytical_precision.json\", \"w\") as outfile: \n",
    "    json.dump(analytical_precision, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field heterogeneity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get field duplicate-parent pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_duplicates = xrf_data[xrf_data[\"qaqc_type\"]==\"field duplicate\"]\n",
    "field_parent_indexes = []\n",
    "\n",
    "for row in field_duplicates.iterrows(): \n",
    "        field_dup_id = row[1][\"sample_id\"]\n",
    "        field_parent_id = field_dup_id.rstrip('F')\n",
    "\n",
    "        condition = xrf_data[\"sample_id\"] == field_parent_id\n",
    "        index = xrf_data.index[condition][0]\n",
    "        field_parent_indexes.append(index)\n",
    "\n",
    "field_parents = xrf_data.iloc[field_parent_indexes]\n",
    "\n",
    "field_pairs = pd.concat([field_duplicates, field_parents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate difference between each duplicate-parent pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements_dup = get_elements(xrf_data.columns.to_list())\n",
    "\n",
    "field_pair_diffs = field_parents.copy()\n",
    "field_pair_diffs.loc[:, elements_dup] = field_pair_diffs.loc[:, elements_dup].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "field_duplicates = field_duplicates.copy()\n",
    "field_duplicates.loc[:, elements_dup] = field_duplicates.loc[:, elements_dup].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "for row in field_pair_diffs.iterrows():\n",
    "    parent_id = row[1][\"sample_id\"]\n",
    "    duplicate_id = parent_id + \"F\"\n",
    "\n",
    "    parent_entry = row[1].to_frame().transpose()\n",
    "    parent_entry.reset_index(inplace=True)\n",
    "    parent_values = parent_entry.loc[:, elements_dup]\n",
    "\n",
    "    duplicate_entry = field_duplicates.loc[field_duplicates[\"sample_id\"]==duplicate_id]\n",
    "    duplicate_entry.reset_index(inplace=True)\n",
    "    duplicate_values = duplicate_entry.loc[:, elements_dup]\n",
    "    \n",
    "    condition = field_pair_diffs[\"sample_id\"] == parent_id\n",
    "    field_pair_diffs.loc[condition, elements_dup] = parent_values.subtract(duplicate_values).values\n",
    "\n",
    "\n",
    "field_pair_diffs = field_pair_diffs[[\"sample_id\"] + elements_dup]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate field heterogeneity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_heterogeneity = {}\n",
    "for element in elements_dup: \n",
    "    diffs = field_pair_diffs[element]\n",
    "    \n",
    "    if False in list(diffs.isnull()): # ensure that there are >= 1 valid data values\n",
    "        diffs.dropna(inplace=True) \n",
    "\n",
    "        if len(diffs) >=3: # min. for Dixon's Q Test \n",
    "            diffs = dixon_test(list(diffs), left=True, right=True, confidence_level=95)[0]\n",
    "        std_dev, n = std_dev_from_pairs(diffs)\n",
    "        mean = xrf_data_bdl_replaced.loc[xrf_data_bdl_replaced[\"qaqc_type\"] != \"standard\", element].mean()\n",
    "\n",
    "\n",
    "        if element in analytical_precision.keys(): # analytical precision must be calculated to determine field heterogeneity\n",
    "            field_heterogeneity[element] = {}\n",
    "            field_heterogeneity[element][\"heterogeneity\"] = std_dev - analytical_precision[element][\"standard deviation\"]\n",
    "            field_heterogeneity[element][\"relative heterogeneity\"] = 100 * field_heterogeneity[element][\"heterogeneity\"] / mean\n",
    "            field_heterogeneity[element][\"number of pairs\"] = n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save field heterogeneity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/interim/field_heterogeneity.json\", \"w\") as outfile: \n",
    "    json.dump(field_heterogeneity, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha= 0.05\n",
    "# element = \"Ba\"\n",
    "# normality_test_data = xrf_data.loc[xrf_data[\"qaqc_type\"]!=\"standard\", element]\n",
    "# k, p = stats.normaltest(normality_test_data)\n",
    "# if p < alpha:\n",
    "#     print(\"reject null hypothesis that sample is normall distributed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export data to csv file\n",
    "xrf_data.to_csv('../data/interim/xrf_data_calib.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # xrf_data.to_excel('../data/interim/xrf_data_calib.xlsx', na_rep=\"NaN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_evaluation = dict.fromkeys(get_elements(xrf_data.columns))\n",
    "for element in dataset_evaluation.keys(): \n",
    "    dataset_evaluation[element] = {\"calibration\":          {}, \\\n",
    "                                   \"analytical precision\": {}, \\\n",
    "                                   \"field heterogeneity\":  {}}\n",
    "\n",
    "for element in reg.keys(): \n",
    "    dataset_evaluation[element][\"calibration\"] = {\"R-squared\": round(reg[element][\"score\"], 2)}\n",
    "\n",
    "for element in analytical_precision.keys(): \n",
    "    dataset_evaluation[element][\"analytical precision\"] = {\"standard deviation\": round(analytical_precision[element][\"standard deviation\"], 2), \\\n",
    "                                                           \"rsd\":                round(analytical_precision[element][\"rsd\"],                2), \\\n",
    "                                                           \"number of pairs\":    round(analytical_precision[element][\"number of pairs\"],    2)}\n",
    "\n",
    "for element in field_heterogeneity.keys(): \n",
    "    dataset_evaluation[element][\"field heterogeneity\"] = {\"heterogeneity\":   round(field_heterogeneity[element][\"heterogeneity\"],   2), \\\n",
    "                                                          \"number of pairs\": round(field_heterogeneity[element][\"number of pairs\"], 2)}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "discard = {}\n",
    "usable_dataset_evaluation = dataset_evaluation.copy()\n",
    "for element in dataset_evaluation.keys(): \n",
    "    if dataset_evaluation[element][\"calibration\"] == {}:\n",
    "        discard[element] = \"srm values missing\"\n",
    "\n",
    "    elif dataset_evaluation[element][\"calibration\"][\"R-squared\"] <= 0.3:\n",
    "        discard[element] = \"poor calibration\"\n",
    "\n",
    "    elif dataset_evaluation[element][\"analytical precision\"] == {}: \n",
    "        discard[element] = \"insufficient data to calculate analytical precision\"\n",
    "        \n",
    "    elif dataset_evaluation[element][\"analytical precision\"][\"number of pairs\"] < 9:\n",
    "        discard[element] = \"insufficient data to calculate analytical precision\"\n",
    "    \n",
    "    elif dataset_evaluation[element][\"analytical precision\"][\"rsd\"] > 30:\n",
    "        discard[element] = \"RSD is too high\"\n",
    "\n",
    "for element in discard.keys():\n",
    "    usable_dataset_evaluation.pop(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean analysis-ready dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrf_data = xrf_data.applymap(half_detection_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da7f753086fad63f468c7afa1043fdfff877310ce7148e2af11ff3397961b305"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('EPSC-552-Mont-St-Hilaire': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
