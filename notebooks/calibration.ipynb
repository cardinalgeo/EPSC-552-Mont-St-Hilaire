{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import json\n",
    "from scipy.stats import linregress\n",
    "from statsmodels.stats.weightstats import ztest as ztest\n",
    "\n",
    "# local code\n",
    "from source.interactive_plots import interactive_linear_regression_calibration_plot\n",
    "from source.get_elements      import get_elements\n",
    "from source.outliers          import dixon_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data from XRF analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "xrf_data = pd.read_csv(\"../data/interim/xrf_data_clean.csv\") # load xrf data\n",
    "# drop uncertainty columns \n",
    "xrf_data.drop([column for column in xrf_data.columns if column.endswith(\"+/-\")], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data for standard reference materials and clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "srm_data = pd.read_csv(\"../data/interim/standard_reference_material_certified_values.csv\") # load SRM data\n",
    "srm_data.drop([\"1SD\", \"95% Confidence Low\", \"95% Confidence High\"], axis=1, inplace=True) # drop unnecessary columns\n",
    "\n",
    "# clean SRM data\n",
    "for row in srm_data.iterrows(): \n",
    "    row[1][\"Sample ID\"] = row[1][\"Sample ID\"].lower()\n",
    "    row[1][\"Analyte\"] = row[1][\"Analyte\"].split(\",\",1)[0]\n",
    "\n",
    "    # clean certified value\n",
    "    cert_val = row[1][\"Certified Value\"]\n",
    "    if cert_val.startswith(\"<\"): \n",
    "        row[1][\"Certified Value\"] = float(cert_val.lstrip(\"< \")) / 2 # replace BDL with half value\n",
    "    else:\n",
    "        row[1][\"Certified Value\"] = float(row[1][\"Certified Value\"])\n",
    "\n",
    "    # clean units\n",
    "    row[1][\"Units\"] = row[1][\"Units\"].lstrip(\"(\").rstrip(\")\")\n",
    "    if row[1][\"Units\"] == \"wt.%\":  # convert units\n",
    "       row[1][\"Certified Value\"] = row[1][\"Certified Value\"] * 1e4\n",
    "       row[1][\"Units\"] = \"ppm\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove standard data for a given element that is unsuitable for calibration\n",
    "\n",
    "Compare the measured concentration of each element for a given standard to the distribution of measured concentrations for that element across all non-standards. If the measured concentration \n",
    "\n",
    "    1.) falls outside the range of the distribution \n",
    "    2.) is more than three standard deviations from the mean \n",
    "\n",
    "that value is set to `NaN` (all values set to `NaN` are filtered out during calibration). \n",
    "\n",
    "Note: I should consider conducting a test to determine if each distribution is normally or log-normally distributed; if the latter, I should log-transform the data and then calculate the z-score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score(data, value): \n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "\n",
    "    return (value - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_standards_data = xrf_data.loc[xrf_data[\"qaqc_type\"]!=\"standard\"]\n",
    "\n",
    "outlier_stddev_cutoff = 5\n",
    "\n",
    "for standard in xrf_data.loc[xrf_data[\"qaqc_type\"]==\"standard\", \"sample_id\"].unique(): \n",
    "    for date in xrf_data.loc[xrf_data[\"sample_id\"]==standard, \"date\"].unique():\n",
    "        for element in get_elements(xrf_data.columns.to_list()): \n",
    "            standard_date_element = xrf_data.loc[(xrf_data[\"sample_id\"]==standard) & (xrf_data[\"date\"]==date)][element].values\n",
    "\n",
    "            if (non_standards_data[element] > standard_date_element[0]).all() | \\\n",
    "               (non_standards_data[element] < standard_date_element[0]).all(): \n",
    "                \n",
    "                # print(non_standards_data[element].to_numpy())\n",
    "                z = z_score(non_standards_data[element], standard_date_element[0])\n",
    "                if abs(z) > outlier_stddev_cutoff: \n",
    "                    # print(standard + \", \" + date + \", \" + element + \", \" + str(standard_date_element[0]) + \", \" + str(z))\n",
    "\n",
    "                    xrf_data.loc[(xrf_data[\"sample_id\"]==standard) & (xrf_data[\"date\"]==date), element] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the elements analyzed by the XRF and for which concentrations are reported for one or more standard reference materials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = get_elements(\n",
    "                        list(\n",
    "                            set(srm_data[\"Analyte\"].unique()) & \\\n",
    "                            set(xrf_data.columns.to_list())\n",
    "                            )\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a linear regression model for each element in order to predict the true concentration from the measured concentration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/robertcollar/miniconda3/envs/EPSC-552-Mont-St-Hilaire/lib/python3.9/site-packages/scipy/stats/_stats_mstats_common.py:170: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in double_scalars\n",
      "\n",
      "/Users/robertcollar/miniconda3/envs/EPSC-552-Mont-St-Hilaire/lib/python3.9/site-packages/scipy/stats/_stats_mstats_common.py:187: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in double_scalars\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# initialize dictionary to hold lin. reg. models for each element\n",
    "reg = {}\n",
    "for element in elements: \n",
    "\n",
    "    # get IDs of standard reference materials\n",
    "    srm = srm_data.loc[srm_data[\"Analyte\"]==element][\"Sample ID\"].unique() \n",
    "\n",
    "    ## TRAIN \n",
    "    # limit training data to standards for which we have standard reference material info for the element at hand\n",
    "    data_train = xrf_data.loc[(xrf_data[\"qaqc_type\"]==\"standard\") & (xrf_data[\"sample_id\"].isin(srm))]\n",
    "    data_train = data_train.dropna(subset=[element]) # change to true condition statement\n",
    "\n",
    "    x_train = [srm_data.loc[\n",
    "                            (srm_data[\"Sample ID\"]==sample) & \\\n",
    "                            (srm_data[\"Analyte\"]==element)\n",
    "                            ][\"Certified Value\"].values[0] for sample in data_train[\"sample_id\"]]\n",
    "    x_train = np.array(x_train)\n",
    "    y_train = data_train[element].to_numpy()\n",
    "\n",
    "    model = linregress(x_train, y_train) # fit linear regression model\n",
    "\n",
    "    if model.slope != 0: #only use calibration curve if meaningful (i.e., if variance in dep. var. explained by variance in indep. var.)\n",
    "        # invert model so that measured concentration is independent var. and true concentration is dependent var. (i.e., y = m*x + b --> x = (1/m)*y - (b/m))\n",
    "        intercept_inv = -model.intercept / model.slope\n",
    "        slope_inv = model.slope ** -1\n",
    "\n",
    "        ## PREDICT (i.e., calibrate)\n",
    "        data_predict = xrf_data.loc[xrf_data[\"qaqc_type\"]!=\"standard\"]\n",
    "        data_predict = data_predict.dropna(subset=[element])\n",
    "\n",
    "        x_predict = data_predict[element]\n",
    "        x_predict = x_predict.to_numpy()\n",
    "        y_predict = slope_inv * x_predict + intercept_inv\n",
    "\n",
    "        ## Save model results\n",
    "        reg[element] = {} # initialize empty dict to save model results\n",
    "\n",
    "        reg[element][\"model\"]                 = model\n",
    "        reg[element][\"x_train\"]               = x_train\n",
    "        reg[element][\"y_train\"]               = y_train\n",
    "        reg[element][\"score\"]                 = model.rvalue\n",
    "        reg[element][\"y-intercept std error\"] = model.intercept_stderr\n",
    "        reg[element][\"slope_inv\"]             = slope_inv\n",
    "        reg[element][\"intercept_inv\"]         = intercept_inv\n",
    "        reg[element][\"x_predict\"]             = x_predict\n",
    "        reg[element][\"y_predict\"]             = y_predict\n",
    "        \n",
    "        xrf_data.loc[xrf_data[\"qaqc_type\"]!=\"standard\", element] = y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the results of the calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e88af73e4dd442a8245fc304af25e95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Elements', options=('Ag', 'As', 'Ba', 'Bi', 'Ca', 'Cd', 'Câ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dropdown_buttons = {\n",
    "    \"data\": \n",
    "        {\n",
    "            \"name\": \"Elements\", \n",
    "            \"columns\": list(reg.keys())\n",
    "        }\n",
    "    }\n",
    "\n",
    "interactive_linear_regression_calibration_plot(dropdown_buttons, reg, x_axis_label=\"True concentration (ppm)\", y_axis_label=\"Measured concentration (ppm)\", title=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the detection limit for each element. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in elements: \n",
    "    detection_limit = reg[element][\"slope_inv\"] * (reg[element][\"model\"].intercept + 3*reg[element][\"y-intercept std error\"]) + reg[element][\"intercept_inv\"]\n",
    "    reg[element][\"detection_limit\"] = detection_limit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply detection limit to dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in reg.keys(): \n",
    "    limit = reg[element][\"detection_limit\"]\n",
    "    xrf_data[element].where(xrf_data[element] >= limit, other=f\"<{limit}\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_duplicates = xrf_data[xrf_data[\"qaqc_type\"]==\"lab duplicate\"]\n",
    "\n",
    "lab_parent_indexes = []\n",
    "\n",
    "for row in lab_duplicates.iterrows(): \n",
    "    lab_dup_id = row[1][\"sample_id\"]\n",
    "    lab_parent_id = lab_dup_id.rstrip('L')\n",
    "\n",
    "    condition = xrf_data[\"sample_id\"] == lab_parent_id\n",
    "    index = xrf_data.index[condition][0]\n",
    "    lab_parent_indexes.append(index)\n",
    "\n",
    "lab_parents = xrf_data.iloc[lab_parent_indexes]\n",
    "\n",
    "lab_pairs = pd.concat([lab_duplicates, lab_parents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def half_detection_limit(x): \n",
    "    if isinstance(x, str): \n",
    "        if x.startswith(\"<\"): \n",
    "            x = float(x.lstrip(\"< \")) / 2 # replace BDL with half value\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_pair_diffs = lab_parents.copy()\n",
    "\n",
    "elements_dup = get_elements(xrf_data.columns.to_list())\n",
    "\n",
    "lab_pair_diffs.loc[:, elements_dup] = lab_pair_diffs.loc[:, elements_dup].apply(pd.to_numeric, errors=\"coerce\")\n",
    "lab_duplicates = lab_duplicates.copy()\n",
    "lab_duplicates.loc[:, elements_dup] = lab_duplicates.loc[:, elements_dup].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "for row in lab_pair_diffs.iterrows():\n",
    "    parent_id = row[1][\"sample_id\"]\n",
    "    duplicate_id = parent_id + \"L\"\n",
    "\n",
    "    parent_entry = row[1].to_frame().transpose()\n",
    "    parent_entry.reset_index(inplace=True)\n",
    "    parent_values = parent_entry.loc[:, elements_dup]\n",
    "\n",
    "    duplicate_entry = lab_duplicates.loc[lab_duplicates[\"sample_id\"]==duplicate_id]\n",
    "    duplicate_entry.reset_index(inplace=True)\n",
    "    duplicate_values = duplicate_entry.loc[:, elements_dup]\n",
    "    \n",
    "    condition = lab_pair_diffs[\"sample_id\"] == parent_id\n",
    "    lab_pair_diffs.loc[condition, elements_dup] = parent_values.subtract(duplicate_values).values\n",
    "\n",
    "    parent_values = parent_values.applymap(half_detection_limit)\n",
    "    duplicate_values = duplicate_values.applymap(half_detection_limit)\n",
    "    pair_values = pd.concat((parent_values, duplicate_values))\n",
    "\n",
    "    condition = xrf_data[\"sample_id\"] == parent_id\n",
    "    xrf_data.loc[condition, elements_dup] = pair_values.mean().values\n",
    "\n",
    "    condition = xrf_data[\"sample_id\"] == duplicate_id\n",
    "    xrf_data.drop(xrf_data[condition].index, inplace=True)\n",
    "xrf_data.reset_index(inplace=True)\n",
    "\n",
    "lab_pair_diffs = lab_pair_diffs[[\"sample_id\"] + elements_dup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_dev_from_pairs(diffs): \n",
    "    diffs = np.asarray(diffs)\n",
    "    n = diffs.size\n",
    "    \n",
    "    std_dev = np.sqrt(np.sum(np.square(diffs)) / (2 * n))\n",
    "\n",
    "    return std_dev, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s5/y_27w4ms6tj21xxv0z3tg3cw0000gn/T/ipykernel_4712/812834540.py:5: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in double_scalars\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analytical_precision = {}\n",
    "for element in elements_dup: \n",
    "    diffs = lab_pair_diffs[element]\n",
    "    diffs.dropna(inplace=True)\n",
    "    if (len(diffs) >=3) & (False in list(lab_pair_diffs[element].isnull())): # min. for Dixon's Q Test & remove elements w/ all null values\n",
    "        diffs = dixon_test(list(diffs), left=True, right=True, confidence_level=95)[0]\n",
    "    std_dev, n = std_dev_from_pairs(diffs)\n",
    "\n",
    "    if not np.isnan(std_dev): \n",
    "        analytical_precision[element] = {}\n",
    "        analytical_precision[element][\"standard deviation\"] = std_dev\n",
    "        analytical_precision[element][\"number of pairs\"] = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/interim/analytical_precision.json\", \"w\") as outfile: \n",
    "    json.dump(analytical_precision, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43, 90, 38, 162, 99, 153, 57, 94, 186, 124, 165, 170]\n"
     ]
    }
   ],
   "source": [
    "field_duplicates = xrf_data[xrf_data[\"qaqc_type\"]==\"field duplicate\"]\n",
    "field_parent_indexes = []\n",
    "\n",
    "for row in field_duplicates.iterrows(): \n",
    "        field_dup_id = row[1][\"sample_id\"]\n",
    "        field_parent_id = field_dup_id.rstrip('F')\n",
    "\n",
    "        condition = xrf_data[\"sample_id\"] == field_parent_id\n",
    "        index = xrf_data.index[condition][0]\n",
    "        field_parent_indexes.append(index)\n",
    "print(field_parent_indexes)\n",
    "field_parents = xrf_data.iloc[field_parent_indexes]\n",
    "\n",
    "field_pairs = pd.concat([field_duplicates, field_parents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>U</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;4.808030478491732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;4.808030478491732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;4.808030478491732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;4.808030478491732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;4.808030478491732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;4.808030478491732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;4.808030478491732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>&lt;4.808030478491732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>&lt;4.808030478491732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>&lt;4.808030478491732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>14.123456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8.493699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>14.368228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15.347316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>&lt;4.808030478491732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10.084717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>9.472787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>&lt;4.808030478491732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>&lt;4.808030478491732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>7.759382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>7.392224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4.822118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8.493699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>16.142825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>8.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>&lt;4.808030478491732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>7.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>&lt;4.808030478491732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>&lt;4.808030478491732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>12.042893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>25.627743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>&lt;4.808030478491732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>&lt;4.808030478491732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>&lt;4.808030478491732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>&lt;4.808030478491732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>&lt;4.808030478491732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>&lt;4.808030478491732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>7.025066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>&lt;4.808030478491732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>&lt;4.808030478491732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>&lt;4.808030478491732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>&lt;4.808030478491732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>&lt;4.808030478491732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     U\n",
       "0   <4.808030478491732\n",
       "1   <4.808030478491732\n",
       "2   <4.808030478491732\n",
       "3                 10.3\n",
       "4   <4.808030478491732\n",
       "5                  9.7\n",
       "6   <4.808030478491732\n",
       "7                  6.9\n",
       "8   <4.808030478491732\n",
       "9   <4.808030478491732\n",
       "10                 7.2\n",
       "11  <4.808030478491732\n",
       "12  <4.808030478491732\n",
       "13                 5.9\n",
       "14                 6.2\n",
       "15  <4.808030478491732\n",
       "16           14.123456\n",
       "17            8.493699\n",
       "18           14.368228\n",
       "19           15.347316\n",
       "20  <4.808030478491732\n",
       "21           10.084717\n",
       "22            9.472787\n",
       "23  <4.808030478491732\n",
       "24  <4.808030478491732\n",
       "25            7.759382\n",
       "26            7.392224\n",
       "27            4.822118\n",
       "28            8.493699\n",
       "29           16.142825\n",
       "30                 8.6\n",
       "31  <4.808030478491732\n",
       "32                 7.6\n",
       "33  <4.808030478491732\n",
       "34  <4.808030478491732\n",
       "35           12.042893\n",
       "36           25.627743\n",
       "37                 NaN\n",
       "38  <4.808030478491732\n",
       "39  <4.808030478491732\n",
       "40  <4.808030478491732\n",
       "41  <4.808030478491732\n",
       "42  <4.808030478491732\n",
       "43  <4.808030478491732\n",
       "44            7.025066\n",
       "45  <4.808030478491732\n",
       "46  <4.808030478491732\n",
       "47  <4.808030478491732\n",
       "48  <4.808030478491732\n",
       "49  <4.808030478491732"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_pair_diffs\n",
    "xrf_data[[\"U\"]].head(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "elements_dup = get_elements(xrf_data.columns.to_list())\n",
    "\n",
    "field_pair_diffs = field_parents.copy()\n",
    "field_pair_diffs.loc[:, elements_dup] = field_pair_diffs.loc[:, elements_dup].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "field_duplicates = field_duplicates.copy()\n",
    "field_duplicates.loc[:, elements_dup] = field_duplicates.loc[:, elements_dup].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "for row in field_pair_diffs.iterrows():\n",
    "    parent_id = row[1][\"sample_id\"]\n",
    "    duplicate_id = parent_id + \"F\"\n",
    "\n",
    "    parent_entry = row[1].to_frame().transpose()\n",
    "    parent_entry.reset_index(inplace=True)\n",
    "    parent_values = parent_entry.loc[:, elements_dup]\n",
    "\n",
    "    duplicate_entry = field_duplicates.loc[field_duplicates[\"sample_id\"]==duplicate_id]\n",
    "    duplicate_entry.reset_index(inplace=True)\n",
    "    duplicate_values = duplicate_entry.loc[:, elements_dup]\n",
    "    \n",
    "    condition = field_pair_diffs[\"sample_id\"] == parent_id\n",
    "    field_pair_diffs.loc[condition, elements_dup] = parent_values.subtract(duplicate_values).values\n",
    "\n",
    "    # parent_values = parent_values.applymap(half_detection_limit)\n",
    "    # duplicate_values = duplicate_values.applymap(half_detection_limit)\n",
    "    # pair_values = pd.concat((parent_values, duplicate_values))\n",
    "\n",
    "    # condition = xrf_data[\"sample_id\"] == parent_id\n",
    "    # xrf_data.loc[condition, elements_dup] = pair_values.mean().values\n",
    "\n",
    "    # condition = xrf_data[\"sample_id\"] == duplicate_id\n",
    "    # xrf_data.drop(xrf_data[condition].index, inplace=True)\n",
    "# xrf_data.reset_index(inplace=True)\n",
    "\n",
    "field_pair_diffs = field_pair_diffs[[\"sample_id\"] + elements_dup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s5/y_27w4ms6tj21xxv0z3tg3cw0000gn/T/ipykernel_4712/812834540.py:5: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in double_scalars\n",
      "\n"
     ]
    }
   ],
   "source": [
    "field_heterogeneity = {}\n",
    "for element in elements_dup: \n",
    "    diffs =field_pair_diffs[element]\n",
    "    diffs.dropna(inplace=True)\n",
    "    if (len(diffs) >=3) & (False in field_pair_diffs[\"Cd\"].isnull()): # min. for Dixon's Q Test & remove elements w/ all null values\n",
    "        diffs = dixon_test(list(diffs), left=True, right=True, confidence_level=95)[0]\n",
    "    std_dev, n = std_dev_from_pairs(diffs)\n",
    "\n",
    "    if not np.isnan(std_dev): \n",
    "        field_heterogeneity[element] = {}\n",
    "        field_heterogeneity[element][\"standard deviation\"] = analytical_precision[element][\"standard deviation\"]\n",
    "        field_heterogeneity[element][\"heterogeneity\"] = std_dev - analytical_precision[element][\"standard deviation\"]\n",
    "        field_heterogeneity[element][\"number of pairs\"] = n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/interim/field_heterogeneity.json\", \"w\") as outfile: \n",
    "    json.dump(field_heterogeneity, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha= 0.05\n",
    "# element = \"Ba\"\n",
    "# normality_test_data = xrf_data.loc[xrf_data[\"qaqc_type\"]!=\"standard\", element]\n",
    "# k, p = stats.normaltest(normality_test_data)\n",
    "# if p < alpha:\n",
    "#     print(\"reject null hypothesis that sample is normall distributed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export data to csv file\n",
    "xrf_data.to_csv('../data/interim/xrf_data_calib.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # xrf_data.to_excel('../data/interim/xrf_data_calib.xlsx', na_rep=\"NaN\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da7f753086fad63f468c7afa1043fdfff877310ce7148e2af11ff3397961b305"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('EPSC-552-Mont-St-Hilaire': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
